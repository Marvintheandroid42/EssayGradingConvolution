{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ‚úèÔ∏è Using Convolution Neural Networks to Grade Essaysüìà\n\n#### Application of image processing methods in natural language processing\n\nThis project uses one dimensional convolutions in order to predict scores for IELTS essay responses through a regression model.","metadata":{}},{"cell_type":"markdown","source":"### Dependencies","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\ndf = pd.read_csv(\"/kaggle/input/ielts-writing-scored-essays-dataset/ielts_writing_dataset.csv\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-15T20:35:06.292282Z","iopub.execute_input":"2023-09-15T20:35:06.292660Z","iopub.status.idle":"2023-09-15T20:35:06.333041Z","shell.execute_reply.started":"2023-09-15T20:35:06.292631Z","shell.execute_reply":"2023-09-15T20:35:06.332052Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing ","metadata":{}},{"cell_type":"code","source":"df.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T20:35:12.564938Z","iopub.execute_input":"2023-09-15T20:35:12.565321Z","iopub.status.idle":"2023-09-15T20:35:12.581644Z","shell.execute_reply.started":"2023-09-15T20:35:12.565290Z","shell.execute_reply":"2023-09-15T20:35:12.580455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing Outliers","metadata":{}},{"cell_type":"code","source":"lengths = [len(i) for i in df[\"Essay\"]]\n\nsns.boxplot(lengths)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T20:35:14.137356Z","iopub.execute_input":"2023-09-15T20:35:14.137798Z","iopub.status.idle":"2023-09-15T20:35:14.416740Z","shell.execute_reply.started":"2023-09-15T20:35:14.137760Z","shell.execute_reply":"2023-09-15T20:35:14.415854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lengthFilter(arr, maxLength):\n    returnArr = []\n    for i in range(len(arr)):\n        if len(arr[i]) < maxLength:\n            returnArr.append(i)    \n    \n    return np.array(returnArr)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T20:35:15.715114Z","iopub.execute_input":"2023-09-15T20:35:15.716343Z","iopub.status.idle":"2023-09-15T20:35:15.722560Z","shell.execute_reply.started":"2023-09-15T20:35:15.716292Z","shell.execute_reply":"2023-09-15T20:35:15.721343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filteredLength = lengthFilter(df[\"Essay\"], 3000)\n\nX = np.array(df[\"Essay\"])[filteredLength]\ny = np.array(df[\"Overall\"])[filteredLength]\n","metadata":{"execution":{"iopub.status.busy":"2023-09-15T20:35:17.778656Z","iopub.execute_input":"2023-09-15T20:35:17.779023Z","iopub.status.idle":"2023-09-15T20:35:17.792600Z","shell.execute_reply.started":"2023-09-15T20:35:17.778992Z","shell.execute_reply":"2023-09-15T20:35:17.791496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lengths = [len(i) for i in X]\n\nsns.boxplot(lengths)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T20:35:19.783127Z","iopub.execute_input":"2023-09-15T20:35:19.783795Z","iopub.status.idle":"2023-09-15T20:35:19.984782Z","shell.execute_reply.started":"2023-09-15T20:35:19.783760Z","shell.execute_reply":"2023-09-15T20:35:19.983888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Encoding and Zero Padding\n\n- Charecter level encoding of words into a 1 dimensional vector using a vocabulary of size 166\n- Zero padding the vectors to create array of uniform dimension","metadata":{}},{"cell_type":"code","source":"vocab = list(sorted(set(np.sum(X))))\nvocab = {vocab[i]:i for i in range(len(vocab))}","metadata":{"execution":{"iopub.status.busy":"2023-09-15T20:35:20.882413Z","iopub.execute_input":"2023-09-15T20:35:20.882773Z","iopub.status.idle":"2023-09-15T20:35:21.164258Z","shell.execute_reply.started":"2023-09-15T20:35:20.882743Z","shell.execute_reply":"2023-09-15T20:35:21.163178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenizeZeroPadding(text):\n    averageLength = []\n    textNums = []\n    for i in range(len(text)):\n        nums = [vocab[j] for j in list(text[i])]\n        averageLength.append(len(nums))\n        numsLength = len(nums)\n\n        missingElements = 3000 - numsLength #make sure that the right amount of zeros are being added\n        zeroArray = list(np.zeros(missingElements))\n        nums = nums + zeroArray\n        textNums.append(nums)\n        \n    return averageLength, textNums","metadata":{"execution":{"iopub.status.busy":"2023-09-15T20:35:22.867004Z","iopub.execute_input":"2023-09-15T20:35:22.868035Z","iopub.status.idle":"2023-09-15T20:35:22.875140Z","shell.execute_reply.started":"2023-09-15T20:35:22.867988Z","shell.execute_reply":"2023-09-15T20:35:22.873905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"averageLength, textNums = tokenizeZeroPadding(X)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T20:35:24.949941Z","iopub.execute_input":"2023-09-15T20:35:24.950332Z","iopub.status.idle":"2023-09-15T20:35:25.420417Z","shell.execute_reply.started":"2023-09-15T20:35:24.950299Z","shell.execute_reply":"2023-09-15T20:35:25.419452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = np.array(textNums)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T20:35:26.072601Z","iopub.execute_input":"2023-09-15T20:35:26.072967Z","iopub.status.idle":"2023-09-15T20:35:26.077605Z","shell.execute_reply.started":"2023-09-15T20:35:26.072937Z","shell.execute_reply":"2023-09-15T20:35:26.076607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Regualarization of Input Data","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nscalerX = MinMaxScaler()\n\nX = scalerX.fit_transform(X)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T20:35:43.733991Z","iopub.execute_input":"2023-09-15T20:35:43.734942Z","iopub.status.idle":"2023-09-15T20:35:43.774593Z","shell.execute_reply.started":"2023-09-15T20:35:43.734905Z","shell.execute_reply":"2023-09-15T20:35:43.773576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Splitting Dataset","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=42, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T20:35:45.406718Z","iopub.execute_input":"2023-09-15T20:35:45.407986Z","iopub.status.idle":"2023-09-15T20:35:45.432435Z","shell.execute_reply.started":"2023-09-15T20:35:45.407941Z","shell.execute_reply":"2023-09-15T20:35:45.431202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling using Neural Network\n\nUse 1D convolutions to find and encode patterns in the text sequences into filters. This allows for the extraction of higher level features like relationship between letters, words and sentences depending on the filter size\n- In this case, filter size was chosen to be 3 which allows for finding patterns between letters. \n\nEncoding layer allows for further feature extraction through encoding the similarity and differnece of the data using vector embeddings. \n- Not a neccesity in this case, but allows for the model to capture similarity between charecters\n- Embedding layer will retain the sequence length but express every element as a higher dimensional vector in order to represent similarity through spatial proximity\n\nFeature maps of the convolutions are used as input to a deep neural network to perform regression.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\ninputLength = 3000\nvocabSize = len(vocab)\n\nin1 = tf.keras.Input(shape=(inputLength,))\nm = tf.keras.layers.Embedding(input_dim = vocabSize, output_dim=32, input_length=inputLength)(in1) #embedding will retain the sequence length but increase the dimensions of the \n\nm = tf.keras.layers.Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(m)\nm = tf.keras.layers.MaxPooling1D(pool_size=2)(m)\nm = tf.keras.layers.Flatten()(m)\n\n\nm = tf.keras.layers.Dense(16, activation='relu')(m)\nm = tf.keras.layers.Dropout(0.3)(m)\nm = tf.keras.layers.Dense(8, activation='relu')(m)\nm = tf.keras.layers.Dropout(0.3)(m)\nout1 = tf.keras.layers.Dense(1, activation='linear')(m) #multiclass classification\n\nmodel = tf.keras.Model(inputs=in1, outputs=out1)\n\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-09-15T20:35:46.666308Z","iopub.execute_input":"2023-09-15T20:35:46.667519Z","iopub.status.idle":"2023-09-15T20:35:46.770309Z","shell.execute_reply.started":"2023-09-15T20:35:46.667472Z","shell.execute_reply":"2023-09-15T20:35:46.769550Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=tf.keras.optimizers.Adam(), \n              metrics=[tf.keras.metrics.RootMeanSquaredError()])\nearly = tf.keras.callbacks.EarlyStopping(monitor='loss', mode='min', verbose=1)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T20:35:48.191093Z","iopub.execute_input":"2023-09-15T20:35:48.191510Z","iopub.status.idle":"2023-09-15T20:35:48.209836Z","shell.execute_reply.started":"2023-09-15T20:35:48.191480Z","shell.execute_reply":"2023-09-15T20:35:48.208764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.utils.vis_utils import plot_model\nplot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T21:36:43.210393Z","iopub.execute_input":"2023-09-15T21:36:43.210755Z","iopub.status.idle":"2023-09-15T21:36:43.462939Z","shell.execute_reply.started":"2023-09-15T21:36:43.210726Z","shell.execute_reply":"2023-09-15T21:36:43.462006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test), batch_size=5, callbacks=[early], verbose=1)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T20:35:49.383000Z","iopub.execute_input":"2023-09-15T20:35:49.383721Z","iopub.status.idle":"2023-09-15T20:36:09.032746Z","shell.execute_reply.started":"2023-09-15T20:35:49.383684Z","shell.execute_reply":"2023-09-15T20:36:09.031739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.lineplot(history.history['loss'], label=\"loss\")\nsns.lineplot(history.history['val_loss'], label=\"val loss\")\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2023-09-15T20:36:16.865680Z","iopub.execute_input":"2023-09-15T20:36:16.866222Z","iopub.status.idle":"2023-09-15T20:36:17.270914Z","shell.execute_reply.started":"2023-09-15T20:36:16.866183Z","shell.execute_reply":"2023-09-15T20:36:17.269972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation, Visualization and Prediction","metadata":{}},{"cell_type":"code","source":"predictions = model.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T20:36:24.320289Z","iopub.execute_input":"2023-09-15T20:36:24.320665Z","iopub.status.idle":"2023-09-15T20:36:24.450925Z","shell.execute_reply.started":"2023-09-15T20:36:24.320635Z","shell.execute_reply":"2023-09-15T20:36:24.449970Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.sum((y_test - predictions.reshape(-1,))**2) / y_test.shape[0] #Validation Mean Sqaured Error","metadata":{"execution":{"iopub.status.busy":"2023-09-15T20:36:26.924127Z","iopub.execute_input":"2023-09-15T20:36:26.924846Z","iopub.status.idle":"2023-09-15T20:36:26.931610Z","shell.execute_reply.started":"2023-09-15T20:36:26.924811Z","shell.execute_reply":"2023-09-15T20:36:26.930570Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Filter Visualization","metadata":{}},{"cell_type":"code","source":"for layer in model.layers:\n    if \"conv\" in layer.name:\n        filters , bias = layer.get_weights()","metadata":{"execution":{"iopub.status.busy":"2023-09-15T20:36:29.156243Z","iopub.execute_input":"2023-09-15T20:36:29.157222Z","iopub.status.idle":"2023-09-15T20:36:29.170325Z","shell.execute_reply.started":"2023-09-15T20:36:29.157176Z","shell.execute_reply":"2023-09-15T20:36:29.169020Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filters.shape #(filter size, filters per sequence, total number of ","metadata":{"execution":{"iopub.status.busy":"2023-09-15T20:36:30.784240Z","iopub.execute_input":"2023-09-15T20:36:30.784956Z","iopub.status.idle":"2023-09-15T20:36:30.792717Z","shell.execute_reply.started":"2023-09-15T20:36:30.784904Z","shell.execute_reply":"2023-09-15T20:36:30.791622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib import pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2023-09-15T20:37:57.705941Z","iopub.execute_input":"2023-09-15T20:37:57.706325Z","iopub.status.idle":"2023-09-15T20:37:57.710758Z","shell.execute_reply.started":"2023-09-15T20:37:57.706292Z","shell.execute_reply":"2023-09-15T20:37:57.709815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(1, 5)\n\nfor i in range(5):\n    axs[i].imshow(filters[:,:,i].T, interpolation='nearest')\n\nplt.show()\n    ","metadata":{"execution":{"iopub.status.busy":"2023-09-15T21:10:10.265692Z","iopub.execute_input":"2023-09-15T21:10:10.266069Z","iopub.status.idle":"2023-09-15T21:10:10.907850Z","shell.execute_reply.started":"2023-09-15T21:10:10.266035Z","shell.execute_reply":"2023-09-15T21:10:10.905735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The visualiation above is the first 5 filters of 32 filters in the convolution layer. These have the shape of (3, 32) as the filter size is 3 elements with each element of the text sequence having the embedding shape (32,1). \n\nFurther extensions could include trying to visualize the feature maps maps by only using the input, embedding and convlution layers which will create a model that outputs the feature map for an input text sequence. Extracting feature map predictions from the convolution part of the model using input data examples will allow for analysis of the types of features extracted by the model. \n\nInput (3000) -> Embedding (3000,32) -> Convolution (3000,32) -> Output (3000,32)\n\nThe output will be 32 (3000,1) feature maps corresponding to the output of each of the filters (is this the right interpretation?)","metadata":{}},{"cell_type":"markdown","source":"# Further Work\n\n\n- Feature engineer different metrics from the raw text like the kinds of words used, word length, number of sentences ect. in order for find correlations in the data for higher scores. \n- Train hyperparameters including batch size, learning rate.\n- Change filter size to extract word or sentence level information using additional convolution layers. This could include building a stack of convolutions that first find charecter level abstractions and use this to find higher level word and sentence level abstractions.\n- Chaage the model archeture to decrease the loss","metadata":{}}]}